{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rnhondova/attention-learn-to-route/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHWBoJA3KA53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1c7404-2842-4d5c-90e2-08c83261153c"
      },
      "source": [
        "# This ensures that a gpu is being used by the current google colab session.\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Mar 30 17:34:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycwxgcUMAWQQ",
        "outputId": "09c56552-0c85-4a92-fc37-c739f4374edf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLd2Yfk653Gu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76d9183-0615-449f-bf09-168fca6f709a"
      },
      "source": [
        "# This code block is used to access your google drive\n",
        "\n",
        "from google.colab import drive\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO6Ir-cHwqnv",
        "outputId": "f28f6769-0698-4185-e291-9b98de784bed"
      },
      "source": [
        "!git clone https://github.com/Rnhondova/garage.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'garage'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 25277 (delta 5), reused 15 (delta 2), pack-reused 25258\u001b[K\n",
            "Receiving objects: 100% (25277/25277), 58.59 MiB | 25.20 MiB/s, done.\n",
            "Resolving deltas: 100% (18789/18789), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPW7QCr8IK6v",
        "outputId": "b7b07272-8751-40b4-fadc-c7c63b7b3117"
      },
      "source": [
        "%cd garage/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/garage\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFov2CnGyviD",
        "outputId": "6d83372f-9f01-459b-9986-745852127ce7"
      },
      "source": [
        "!git submodule update --init --recursive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submodule 'src/attention-learn-to-route' (https://github.com/Rnhondova/attention-learn-to-route.git) registered for path 'src/attention-learn-to-route'\n",
            "Cloning into '/content/garage/src/attention-learn-to-route'...\n",
            "Submodule path 'src/attention-learn-to-route': checked out 'd522e198446438df6a237ee46f99ffa7552c99f1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnXWqHp0w-y3",
        "outputId": "10e31624-8bbb-4f63-c1b5-a4f12da06c2f"
      },
      "source": [
        "#%cd garage/src/garage/torch/algos/attention-learn-to-route/\n",
        "%cd src/attention-learn-to-route/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/garage/src/attention-learn-to-route\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTNHf8woxkr9",
        "outputId": "5ff1050a-a35f-4fdc-ed27-891fcd475646"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Attention.ipynb\t   nets\t\t\t    run.py\n",
            " environment.yml\t   options.py\t\t    simple_tsp.ipynb\n",
            " eval.py\t\t   plot_vrp.ipynb\t   'train copy.py'\n",
            " garage_requirements.txt   pretrained\t\t    train_copy.py\n",
            " generate_data.py\t   problems\t\t    train.py\n",
            " images\t\t\t   README.md\t\t    TRPO_Attention.py\n",
            " LICENSE\t\t   reinforce_baselines.py   utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqFZroDoBh7T",
        "outputId": "74ae0096-6608-4b8c-ecf6-f690f3530c34"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -r garage_requirements.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.0.1\n",
            "Collecting akro\n",
            "  Downloading akro-0.0.8.tar.gz (17 kB)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 3)) (1.3.0)\n",
            "Collecting cma==2.7.0\n",
            "  Downloading cma-2.7.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[K     |████████████████████████████████| 239 kB 11.5 MB/s \n",
            "\u001b[?25hCollecting dowel==0.0.3\n",
            "  Downloading dowel-0.0.3.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 6)) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 7)) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 8)) (2.8.1)\n",
            "Collecting ray\n",
            "  Downloading ray-1.2.0-cp37-cp37m-manylinux2014_x86_64.whl (47.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.5 MB 77 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 10)) (0.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 11)) (1.4.1)\n",
            "Collecting setproctitle>=1.0\n",
            "  Downloading setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 13)) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-probability>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 14)) (0.12.1)\n",
            "Requirement already satisfied: torch!=1.5.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 15)) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from -r garage_requirements.txt (line 16)) (0.9.1+cu101)\n",
            "Collecting tensorboard_logger\n",
            "  Downloading tensorboard_logger-0.1.0-py2.py3-none-any.whl (17 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.10.23-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 79.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from dowel==0.0.3->-r garage_requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from dowel==0.0.3->-r garage_requirements.txt (line 5)) (0.8.9)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.1-py2.py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 86.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (0.36.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.32.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (3.12.4)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (2.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.12)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.11.0->-r garage_requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.11.0->-r garage_requirements.txt (line 14)) (0.1.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->-r garage_requirements.txt (line 16)) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (54.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.28.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (3.3.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (3.1.0)\n",
            "Requirement already satisfied: gym>=0.12.4 in /usr/local/lib/python3.7/dist-packages (from akro->-r garage_requirements.txt (line 1)) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.12.4->akro->-r garage_requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.12.4->akro->-r garage_requirements.txt (line 1)) (0.16.0)\n",
            "Collecting aioredis\n",
            "  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting opencensus\n",
            "  Downloading opencensus-0.7.12-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 90.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray->-r garage_requirements.txt (line 9)) (0.9.0)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 569 kB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray->-r garage_requirements.txt (line 9)) (1.0.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray->-r garage_requirements.txt (line 9)) (2.6.0)\n",
            "Collecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.5-py2.py3-none-manylinux1_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 83.3 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray->-r garage_requirements.txt (line 9)) (3.13)\n",
            "Collecting colorful\n",
            "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[K     |████████████████████████████████| 201 kB 91.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray->-r garage_requirements.txt (line 9)) (3.0.12)\n",
            "Collecting gpustat\n",
            "  Downloading gpustat-0.6.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r garage_requirements.txt (line 10)) (2.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r garage_requirements.txt (line 10)) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r garage_requirements.txt (line 10)) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->dowel==0.0.3->-r garage_requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->dowel==0.0.3->-r garage_requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->dowel==0.0.3->-r garage_requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r garage_requirements.txt (line 18)) (2.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "  Downloading sentry_sdk-1.0.0-py2.py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 85.7 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.14-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 92.0 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 72.2 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 86.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray->-r garage_requirements.txt (line 9)) (20.3.0)\n",
            "Collecting hiredis\n",
            "  Downloading hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray->-r garage_requirements.txt (line 9)) (7.352.0)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.14->-r garage_requirements.txt (line 13)) (3.4.1)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray->-r garage_requirements.txt (line 9)) (1.26.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray->-r garage_requirements.txt (line 9)) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray->-r garage_requirements.txt (line 9)) (20.9)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray->-r garage_requirements.txt (line 9)) (2018.9)\n",
            "Building wheels for collected packages: dowel, akro, subprocess32, gpustat, pathtools\n",
            "  Building wheel for dowel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dowel: filename=dowel-0.0.3-py3-none-any.whl size=13012 sha256=163f781b783422e49ac2226f490e42665fe25ea73d5162674218b458544905d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/06/28/72e80c0c8be293503952b3a3fbd0902d00a615229b372b2451\n",
            "  Building wheel for akro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for akro: filename=akro-0.0.8-py3-none-any.whl size=11136 sha256=ccfae57485d8def714136bea25bdb3cfcade440314f7b39850b7f34f6e3c3971\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/8b/e3/91fe8a152629ca652b971172afb5600aa666f34cfd64968f68\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6488 sha256=0c73b5db9e7ed5360d25e72e4b6761fe0cad849df3a1f8addd795c05a894c60c\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=279ee94f81f7a2514ef782c58c253fce94f7c8634f09d14945b46eacbc85dd47\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=804689833a37da254b4c3447062b9d686f216094c9fa21085991f582f7a14622\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built dowel akro subprocess32 gpustat pathtools\n",
            "Installing collected packages: multidict, yarl, smmap, async-timeout, opencensus-context, hiredis, gitdb, blessings, aiohttp, tensorboardX, subprocess32, shortuuid, sentry-sdk, redis, py-spy, pathtools, opencensus, gpustat, GitPython, docker-pycreds, configparser, colorful, colorama, aioredis, aiohttp-cors, wandb, tensorboard-logger, setproctitle, ray, dowel, cma, akro\n",
            "Successfully installed GitPython-3.1.14 aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 akro-0.0.8 async-timeout-3.0.1 blessings-1.7 cma-2.7.0 colorama-0.4.4 colorful-0.5.4 configparser-5.0.2 docker-pycreds-0.4.0 dowel-0.0.3 gitdb-4.0.7 gpustat-0.6.0 hiredis-2.0.0 multidict-5.1.0 opencensus-0.7.12 opencensus-context-0.1.2 pathtools-0.1.2 py-spy-0.3.5 ray-1.2.0 redis-3.5.3 sentry-sdk-1.0.0 setproctitle-1.2.2 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 tensorboard-logger-0.1.0 tensorboardX-2.1 wandb-0.10.23 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGZLM66tz6lS",
        "outputId": "08083912-b42b-4ed6-938e-836114cc085a"
      },
      "source": [
        "!python run.py --graph_size 20 \n",
        "\n",
        "\n",
        "--batch_size 512 --problem cvrp --baseline rollout --run_name 'vrp100_rollout' --epoch_size 12800 --n_epochs 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-30 17:39:27.159209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "{'baseline': 'rollout',\n",
            " 'batch_size': 512,\n",
            " 'bl_alpha': 0.05,\n",
            " 'bl_warmup_epochs': 1,\n",
            " 'checkpoint_encoder': False,\n",
            " 'checkpoint_epochs': 1,\n",
            " 'data_distribution': None,\n",
            " 'embedding_dim': 128,\n",
            " 'epoch_size': 12800,\n",
            " 'epoch_start': 0,\n",
            " 'eval_batch_size': 1024,\n",
            " 'eval_only': False,\n",
            " 'exp_beta': 0.8,\n",
            " 'graph_size': 20,\n",
            " 'hidden_dim': 128,\n",
            " 'load_path': None,\n",
            " 'log_dir': 'logs',\n",
            " 'log_step': 50,\n",
            " 'lr_critic': 0.0001,\n",
            " 'lr_decay': 1.0,\n",
            " 'lr_model': 0.0001,\n",
            " 'max_grad_norm': 1.0,\n",
            " 'model': 'attention',\n",
            " 'n_encode_layers': 3,\n",
            " 'n_epochs': 1,\n",
            " 'no_cuda': False,\n",
            " 'no_progress_bar': False,\n",
            " 'no_tensorboard': False,\n",
            " 'normalization': 'batch',\n",
            " 'output_dir': 'outputs',\n",
            " 'problem': 'cvrp',\n",
            " 'resume': None,\n",
            " 'run_name': 'vrp100_rollout_20210330T173931',\n",
            " 'save_dir': 'outputs/cvrp_20/vrp100_rollout_20210330T173931',\n",
            " 'seed': 1234,\n",
            " 'shrink_size': None,\n",
            " 'tanh_clipping': 10.0,\n",
            " 'use_cuda': True,\n",
            " 'val_dataset': None,\n",
            " 'val_size': 10000}\n",
            "Evaluating baseline model on evaluation dataset\n",
            "100% 10/10 [00:01<00:00,  9.32it/s]\n",
            "Start train epoch 0, lr=0.0001 for run vrp100_rollout_20210330T173931\n",
            "  0% 0/25 [00:00<?, ?it/s]---Checking data Sizes---\n",
            "Batch ID: 0 -> loc -> torch.Size([512, 20, 2])\n",
            "Batch ID: 0 -> demand -> torch.Size([512, 20])\n",
            "Batch ID: 0 -> depot -> torch.Size([512, 2])\n",
            "Batch ID: 0 -> Cost -> torch.Size([512])\n",
            "  0% 0/25 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 173, in <module>\n",
            "    run(get_options())\n",
            "  File \"run.py\", line 168, in run\n",
            "    opts\n",
            "  File \"/content/garage/src/attention-learn-to-route/train_copy.py\", line 103, in train_epoch\n",
            "    opts\n",
            "  File \"/content/garage/src/attention-learn-to-route/train_copy.py\", line 193, in train_batch\n",
            "    print('VPG run' + str(vpg._train_once(1, eps)))\n",
            "  File \"/content/garage/src/garage/torch/algos/vpg.py\", line 156, in _train_once\n",
            "    baselines = self._value_function(obs)\n",
            "TypeError: 'NoneType' object is not callable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwHxwKkL6qh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6280537-3c16-4b25-8a6e-695f3c73292c"
      },
      "source": [
        "# Make sure this points to the project folder\n",
        "\n",
        "%cd drive/'My Drive'/CORL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CORL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQXpBJPg0-V1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d1f5ed-e87f-410e-e8b9-eba737d090c6"
      },
      "source": [
        "%cd attention"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CORL/attention\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "DJVJJzi9Wrzb",
        "outputId": "af67a606-70e8-4078-84bc-3864565067db"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cgIK0U5M26a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f1ba7a-aba6-4c30-b2fb-1192a4c9a5e0"
      },
      "source": [
        "# This block will run the originial attention code with the below settings\n",
        "# The save_hrs are the checkpoint hours to save the model\n",
        "\n",
        "!python run.py --graph_size 100 --batch_size 64 --problem cvrp --baseline rollout --run_name 'vrp100_rollout' --save_hrs 5 10 --epoch_size 12800 --n_epochs 500\n",
        "\n",
        "# this is an example of how to run evolution code\n",
        "#!python vrp_evolve.py --save_dir ../models/att_evo --save_hrs 2 3 4 5 6 8 10 --sigma 0.001 --lr 0.000001 --dataset_size 12800 --epochs 500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-26 13:50:45.455415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrnhondova\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "2021-01-26 13:50:59.439484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meternal-haze-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rnhondova/pytorch-attention-model\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rnhondova/pytorch-attention-model/runs/25jujl2f\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/drive/My Drive/CORL/attention/wandb/run-20210126_135052-25jujl2f\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "{'baseline': 'rollout',\n",
            " 'batch_size': 64,\n",
            " 'bl_alpha': 0.05,\n",
            " 'bl_warmup_epochs': 1,\n",
            " 'checkpoint_encoder': False,\n",
            " 'checkpoint_epochs': 1,\n",
            " 'data_distribution': None,\n",
            " 'embedding_dim': 128,\n",
            " 'epoch_size': 12800,\n",
            " 'epoch_start': 0,\n",
            " 'eval_batch_size': 1024,\n",
            " 'eval_only': False,\n",
            " 'exp_beta': 0.8,\n",
            " 'graph_size': 100,\n",
            " 'hidden_dim': 128,\n",
            " 'load_path': None,\n",
            " 'log_dir': 'logs',\n",
            " 'log_step': 50,\n",
            " 'lr_critic': 0.0001,\n",
            " 'lr_decay': 1.0,\n",
            " 'lr_model': 0.0001,\n",
            " 'max_grad_norm': 1.0,\n",
            " 'model': 'attention',\n",
            " 'n_encode_layers': 3,\n",
            " 'n_epochs': 500,\n",
            " 'no_cuda': False,\n",
            " 'no_progress_bar': False,\n",
            " 'no_tensorboard': False,\n",
            " 'normalization': 'batch',\n",
            " 'output_dir': 'outputs',\n",
            " 'problem': 'cvrp',\n",
            " 'resume': None,\n",
            " 'run_name': 'vrp100_rollout_20210126T135052',\n",
            " 'save_dir': 'outputs/cvrp_100/vrp100_rollout_20210126T135052',\n",
            " 'save_hrs': [5, 10],\n",
            " 'seed': 1234,\n",
            " 'shrink_size': None,\n",
            " 'tanh_clipping': 10.0,\n",
            " 'use_cuda': True,\n",
            " 'val_dataset': None,\n",
            " 'val_size': 10000}\n",
            "Evaluating baseline model on evaluation dataset\n",
            "100%|██████████| 10/10 [00:09<00:00,  1.10it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 210, in <module>\n",
            "    run(config)\n",
            "  File \"run.py\", line 167, in run\n",
            "    torch.save(model, os.path.join('.', 'empty.pt'))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 372, in save\n",
            "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 476, in _save\n",
            "    pickler.dump(obj)\n",
            "AttributeError: Can't pickle local object 'TorchGraph.hook_torch_modules.<locals>.backward_hook'\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 372\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1.  Press ctrl-c to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/drive/My Drive/CORL/attention/wandb/run-20210126_135052-25jujl2f/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/drive/My Drive/CORL/attention/wandb/run-20210126_135052-25jujl2f/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33meternal-haze-2\u001b[0m: \u001b[34mhttps://wandb.ai/rnhondova/pytorch-attention-model/runs/25jujl2f\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEU7zEhHUqzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db68e877-7905-4546-f639-53e8a1d29f85"
      },
      "source": [
        "!git status"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mHEAD detached at \u001b[mb1b1b0c\n",
            "Changes not staged for commit:\n",
            "  (use \"git add/rm <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   run.py\u001b[m\n",
            "\t\u001b[31mdeleted:    train copy.py\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mMakefile\u001b[m\n",
            "\t\u001b[31mgarage_requirements.txt\u001b[m\n",
            "\t\u001b[31mtrain_copy.py\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_IWMkKGVbyp"
      },
      "source": [
        "!git add -A"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFZpTaAQVwLR"
      },
      "source": [
        "!git config --global user.email \"rnhondova@yahoo.com\""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFFHnmrXV5xd"
      },
      "source": [
        "!git config --global user.name \"Ronald Nhondova\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQhvqoQIVfQ1",
        "outputId": "c0749741-6613-4824-c1f7-2a9212483c4d"
      },
      "source": [
        "!git commit -m \"Add required files to set garage\""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[detached HEAD 18b3b07] Add required files to set garage\n",
            " 4 files changed, 53 insertions(+), 16 deletions(-)\n",
            " create mode 100644 Makefile\n",
            " create mode 100644 garage_requirements.txt\n",
            " rename train copy.py => train_copy.py (87%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plx8L2EGV_vk",
        "outputId": "f0644d70-a0d8-4155-b7df-063c70c00ca2"
      },
      "source": [
        "!git push origin HEAD:master"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o796PPQWkpLF",
        "outputId": "af349777-091d-49ac-ac1f-961f5fa8ebca"
      },
      "source": [
        "import numpy as np\n",
        "test_ = np.array([1,2,3,4,5,6])\n",
        "test_ = test_.reshape(test_.shape[0],1)\n",
        "for reward in test_:\n",
        "  print(reward[::-1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\n",
            "[2]\n",
            "[3]\n",
            "[4]\n",
            "[5]\n",
            "[6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96-YALbflQCD"
      },
      "source": [
        "import scipy.signal\n",
        "def discount_cumsum(x, discount):\n",
        "    \"\"\"Discounted cumulative sum.\n",
        "\n",
        "    See https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html#difference-equation-filtering  # noqa: E501\n",
        "    Here, we have y[t] - discount*y[t+1] = x[t]\n",
        "    or rev(y)[t] - discount*rev(y)[t-1] = rev(x)[t]\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarrary): Input.\n",
        "        discount (float): Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarrary: Discounted cumulative sum.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1],\n",
        "                                axis=-1)[::-1]\n",
        "                  \n",
        "def pad_tensor(x, max_len, mode='zero'):\n",
        "    \"\"\"Pad tensors.\n",
        "\n",
        "    Args:\n",
        "        x (numpy.ndarray): Tensors to be padded.\n",
        "        max_len (int): Maximum length.\n",
        "        mode (str): If 'last', pad with the last element, otherwise pad with 0.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Padded tensor.\n",
        "\n",
        "    \"\"\"\n",
        "    padding = np.zeros_like(x[0])\n",
        "    if mode == 'last':\n",
        "        padding = x[-1]\n",
        "\n",
        "    return np.concatenate(\n",
        "        [x, np.tile(padding, (max_len - len(x), ) + (1, ) * np.ndim(x[0]))])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyPUzDDhlgn_",
        "outputId": "f288fb84-b8ef-4554-d599-28eb8322b251"
      },
      "source": [
        "for reward in test_:\n",
        "  (reward[::-1])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [2],\n",
              "       [3],\n",
              "       [4],\n",
              "       [5],\n",
              "       [6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Nmc8h2mjgK"
      },
      "source": [
        "import torch\n",
        "rewards = torch.Tensor(test_)\n",
        "returns = torch.Tensor(\n",
        "    np.stack([\n",
        "        discount_cumsum(reward, 0.5)\n",
        "        for reward in test_\n",
        "    ]))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FINI9erenDfK",
        "outputId": "31958b6b-369b-4e18-f23c-1f478f7065c3"
      },
      "source": [
        "returns"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [6.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM7VWjgQt-ps",
        "outputId": "58f339ec-deb0-45f9-aba5-30cd3d360784"
      },
      "source": [
        "pad_tensor(test_, len(test_), mode='last')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [2],\n",
              "       [3],\n",
              "       [4],\n",
              "       [5],\n",
              "       [6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JiRC0KHcHVy",
        "outputId": "b3d386f3-608f-4013-88cc-7338c4049626"
      },
      "source": [
        "[20*20]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[400]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    }
  ]
}